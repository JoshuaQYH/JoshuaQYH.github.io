<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>机器学习 | 决策树 | KnowMyself | QiuYH&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="python,机器学习,数据分析">
    <meta name="description" content="决策树的工作原理定义决策树就是根据已有的规则条件进行评判，给出决策结果。 一棵决策树一般包括根结点，内部结点，和叶节点。根结点是一开始的判断条件，内部结点是中间过程的判断条件，而叶结点则对应着决策结果。 使用决策树时一般会经历两个阶段：构造和剪枝。">
<meta name="keywords" content="python,机器学习,数据分析">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习 | 决策树">
<meta property="og:url" content="https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/index.html">
<meta property="og:site_name" content="KnowMyself">
<meta property="og:description" content="决策树的工作原理定义决策树就是根据已有的规则条件进行评判，给出决策结果。 一棵决策树一般包括根结点，内部结点，和叶节点。根结点是一开始的判断条件，内部结点是中间过程的判断条件，而叶结点则对应着决策结果。 使用决策树时一般会经历两个阶段：构造和剪枝。">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/dc/90/dca4224b342894f12f54a9cb41d8cd90.jpg">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/32/07/327eafa4a33e3e76ca86ac59195c0307.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/f9/89/f9bb4cce5b895499cabc714eb372b089.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/69/9a/69a90a43146898150a0de0811c6fef9a.jpg">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/10/1e/107fed838cb75df62eb149499db20c1e.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/6f/97/6f9677a70b1edff85e9e467f3e52bd97.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/04/c1/045fd5afb7b53f17a8accd6f337f63c1.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/6b/95/6b9735123d45e58f0b0afc7c3f68cd95.png">
<meta property="og:updated_time" content="2019-02-23T20:04:12.231Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习 | 决策树">
<meta name="twitter:description" content="决策树的工作原理定义决策树就是根据已有的规则条件进行评判，给出决策结果。 一棵决策树一般包括根结点，内部结点，和叶节点。根结点是一开始的判断条件，内部结点是中间过程的判断条件，而叶结点则对应着决策结果。 使用决策树时一般会经历两个阶段：构造和剪枝。">
<meta name="twitter:image" content="https://static001.geekbang.org/resource/image/dc/90/dca4224b342894f12f54a9cb41d8cd90.jpg">
    
        <link rel="alternate" type="application/atom+xml" title="KnowMyself" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Qiuyihao</h5>
          <a href="mailto:576261090@qq.com" title="576261090@qq.com" class="mail">576261090@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/JoshuaQYH" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">机器学习 | 决策树</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="検索">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">机器学习 | 决策树</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-02-18T02:18:21.000Z" itemprop="datePublished" class="page-time">
  2019-02-18
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#决策树的工作原理"><span class="post-toc-number">1.</span> <span class="post-toc-text">决策树的工作原理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#定义"><span class="post-toc-number">1.0.1.</span> <span class="post-toc-text">定义</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#构造"><span class="post-toc-number">1.0.2.</span> <span class="post-toc-text">构造</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#剪枝"><span class="post-toc-number">1.0.3.</span> <span class="post-toc-text">剪枝</span></a></li></ol></li></ol><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#决策树构造指标"><span class="post-toc-number">2.</span> <span class="post-toc-text">决策树构造指标</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#决策树构造代码"><span class="post-toc-number">3.</span> <span class="post-toc-text">决策树构造代码</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#CART算法"><span class="post-toc-number">4.</span> <span class="post-toc-text">CART算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#分类决策树"><span class="post-toc-number">4.0.1.</span> <span class="post-toc-text">分类决策树</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#回归决策树"><span class="post-toc-number">4.0.2.</span> <span class="post-toc-text">回归决策树</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#CART-决策树的剪枝"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">CART 决策树的剪枝</span></a></li>
        </nav>
    </aside>


<article id="post-机器学习-决策树" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">机器学习 | 决策树</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-02-18 10:18:21" datetime="2019-02-18T02:18:21.000Z" itemprop="datePublished">2019-02-18</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="决策树的工作原理"><a href="#决策树的工作原理" class="headerlink" title="决策树的工作原理"></a>决策树的工作原理</h1><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>决策树就是根据已有的规则条件进行评判，给出决策结果。</p>
<p>一棵决策树一般包括根结点，内部结点，和叶节点。根结点是一开始的判断条件，内部结点是中间过程的判断条件，而叶结点则对应着决策结果。</p>
<p>使用决策树时一般会经历两个阶段：构造和剪枝。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/dc/90/dca4224b342894f12f54a9cb41d8cd90.jpg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h3 id="构造"><a href="#构造" class="headerlink" title="构造"></a>构造</h3><p>决策树的构造过程就是选择什么样的属性作为决策结点的过程。我们需要经过计算合理构造节点之间的关系。</p>
<p>在构造过程中，要解决三个重要的问题：</p>
<ol>
<li><p>选择哪个属性作为根节点；</p>
</li>
<li><p>选择哪些属性作为子节点；</p>
</li>
<li><p>什么时候停止并得到目标状态，即叶节点。</p>
<p>​</p>
</li>
</ol>
<h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>剪枝相当于给决策瘦身，避免了许多无谓的决策判断，尤其在处理大型决策树的时候，剪枝尤为必要。同时这么做，也是为了防止过拟合的现象产生。</p>
<p>“过拟合”指的就是模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。</p>
<p>造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会<strong>把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点</strong>，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。</p>
<p>泛化能力指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。</p>
<p><strong>剪枝的方法</strong></p>
<ol>
<li>预剪枝（pre-pruning）</li>
<li>后剪枝（post-pruning)</li>
</ol>
<p>预剪枝就是在<strong>决策树开始构造</strong>的时候就进行剪枝，方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中就不能带来准确性的提升，那么对该节点就没有必要进行划分，此时就将当前节点视为叶节点。</p>
<p>后剪枝就是在<strong>生成决策树之后</strong>再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。</p>
<h1 id="决策树构造指标"><a href="#决策树构造指标" class="headerlink" title="决策树构造指标"></a>决策树构造指标</h1><p>如何构造一个根据天气情况来判断是否去打篮球的决策树？以下是一个简单的数据集。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/32/07/327eafa4a33e3e76ca86ac59195c0307.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>接下来进行简单的构造，我们必须思考以下问题：</p>
<ol>
<li>哪个属性划分效果做好，作为根节点？</li>
<li>哪些属性作为后继节点？</li>
<li>什么时候停止划分，得到叶节点？</li>
</ol>
<p>我们使用信息熵（entropy）来作为划分的指标，它表示了信息的不确定性。我们假设随机变量 $I$ 具有值 $i$，$i$ 的概率为$p_i$。</p>
<p>信息熵的数学公式如下：$H(x) = - \sum_{i =1}^{n} p_i log_2p_i$</p>
<p>同样还有其他信息指标：</p>
<ul>
<li><p>conditional entropy（条件熵）: $H(X|Y)=\sum{P(X|Y)}\log_2{P(X|Y)}$</p>
</li>
<li><p>information gain（信息增益） : $g(D, A)=H(D)-H(D|A)$</p>
</li>
<li><p>information gain ratio（信息增益比）: $g_R(D, A) = \frac{g(D,A)}{H(A)}$</p>
</li>
<li><p>gini index（基尼指数）:$Gini(D)=\sum_{k=1}^{K}p_k\log{p_k}=1-\sum_{k=1}^{K}p_k^2$</p>
</li>
</ul>
<p>信息增益越大越好，分类更正确</p>
<p>信息增益越大，信息给你带来的混乱程度越小。</p>
<p>信息论中熵越大，混乱程度越大，信息量越小，信息增益更小。</p>
<p>故信息增益越大的特征，在决策树上的越高。</p>
<p>存在多种决策树：</p>
<ul>
<li>ID3 决策树 （基于信息增益划分）</li>
<li>C4.5 决策树（基于信息增益比）</li>
<li>CART 决策树 （基尼指数 ）</li>
</ul>
<p>ID3算法简单，但解释性强，但存在缺陷，有些属性对分类任务没有太大作用，但是依旧有可能被选为最优属性。</p>
<p>在针对ID3算法，做了改进，于是又了C4.5算法。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。</p>
<p>悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。</p>
<p>C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，<strong>C4.5 选择具有最高信息增益的划分所对应的阈值</strong>。</p>
<p>C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。</p>
<h1 id="决策树构造代码"><a href="#决策树构造代码" class="headerlink" title="决策树构造代码"></a>决策树构造代码</h1> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义节点类 二叉树</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root=True, label=None, feature_name=None, feature=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.label = label</span><br><span class="line">        self.feature_name = feature_name</span><br><span class="line">        self.feature = feature</span><br><span class="line">        self.tree = &#123;&#125;</span><br><span class="line">        self.result = &#123;<span class="string">'label:'</span>: self.label, <span class="string">'feature'</span>: self.feature, <span class="string">'tree'</span>: self.tree&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'&#123;&#125;'</span>.format(self.result)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span><span class="params">(self, val, node)</span>:</span></span><br><span class="line">        self.tree[val] = node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.root <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">return</span> self.label</span><br><span class="line">        <span class="keyword">return</span> self.tree[features[self.feature]].predict(features)</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, epsilon=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self._tree = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 熵</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span><span class="params">(datasets)</span>:</span></span><br><span class="line">        data_length = len(datasets)</span><br><span class="line">        label_count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(data_length):</span><br><span class="line">            label = datasets[i][<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count:</span><br><span class="line">                label_count[label] = <span class="number">0</span></span><br><span class="line">            label_count[label] += <span class="number">1</span></span><br><span class="line">        ent = -sum([(p/data_length)*log(p/data_length, <span class="number">2</span>) <span class="keyword">for</span> p <span class="keyword">in</span> label_count.values()])</span><br><span class="line">        <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 经验条件熵</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cond_ent</span><span class="params">(self, datasets, axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">        data_length = len(datasets)</span><br><span class="line">        feature_sets = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(data_length):</span><br><span class="line">            feature = datasets[i][axis]</span><br><span class="line">            <span class="keyword">if</span> feature <span class="keyword">not</span> <span class="keyword">in</span> feature_sets:</span><br><span class="line">                feature_sets[feature] = []</span><br><span class="line">            feature_sets[feature].append(datasets[i])</span><br><span class="line">        cond_ent = sum([(len(p)/data_length)*self.calc_ent(p) <span class="keyword">for</span> p <span class="keyword">in</span> feature_sets.values()])</span><br><span class="line">        <span class="keyword">return</span> cond_ent</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 信息增益</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span><span class="params">(ent, cond_ent)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ent - cond_ent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain_train</span><span class="params">(self, datasets)</span>:</span></span><br><span class="line">        count = len(datasets[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">        ent = self.calc_ent(datasets)</span><br><span class="line">        best_feature = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(count):</span><br><span class="line">            c_info_gain = self.info_gain(ent, self.cond_ent(datasets, axis=c))</span><br><span class="line">            best_feature.append((c, c_info_gain))</span><br><span class="line">        <span class="comment"># 比较大小</span></span><br><span class="line">        best_ = max(best_feature, key=<span class="keyword">lambda</span> x: x[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> best_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        input:数据集D(DataFrame格式)，特征集A，阈值eta</span></span><br><span class="line"><span class="string">        output:决策树T</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        _, y_train, features = train_data.iloc[:, :<span class="number">-1</span>], train_data.iloc[:, <span class="number">-1</span>], train_data.columns[:<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> len(y_train.value_counts()) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(root=<span class="keyword">True</span>,</span><br><span class="line">                        label=y_train.iloc[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2, 若A为空，则T为单节点树，将D中实例树最大的类Ck作为该节点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> len(features) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(root=<span class="keyword">True</span>, label=y_train.value_counts().sort_values(ascending=<span class="keyword">False</span>).index[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3,计算最大信息增益 同5.1,Ag为信息增益最大的特征</span></span><br><span class="line">        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))</span><br><span class="line">        max_feature_name = features[max_feature]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4,Ag的信息增益小于阈值eta,则置T为单节点树，并将D中是实例数最大的类Ck作为该节点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> max_info_gain &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> Node(root=<span class="keyword">True</span>, label=y_train.value_counts().sort_values(ascending=<span class="keyword">False</span>).index[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5,构建Ag子集</span></span><br><span class="line">        node_tree = Node(root=<span class="keyword">False</span>, feature_name=max_feature_name, feature=max_feature)</span><br><span class="line"></span><br><span class="line">        feature_list = train_data[max_feature_name].value_counts().index</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> feature_list:</span><br><span class="line">            sub_train_df = train_data.loc[train_data[max_feature_name] == f].drop([max_feature_name], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 6, 递归生成树</span></span><br><span class="line">            sub_tree = self.train(sub_train_df)</span><br><span class="line">            node_tree.add_node(f, sub_tree)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pprint.pprint(node_tree.tree)</span></span><br><span class="line">        <span class="keyword">return</span> node_tree</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, train_data)</span>:</span></span><br><span class="line">        self._tree = self.train(train_data)</span><br><span class="line">        <span class="keyword">return</span> self._tree</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_test)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._tree.predict(X_test)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">datasets, labels = create_data()</span><br><span class="line">data_df = pd.DataFrame(datasets, columns=labels)</span><br><span class="line">dt = DTree()</span><br><span class="line">tree = dt.fit(data_df)</span><br></pre></td></tr></table></figure>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>我们使用CART算法来生成一个分类回归决策树。ID3 和 C4.5算法可以生成多叉树，但是CART只支持二叉树。</p>
<p>分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p>
<p>我们使用基尼系数 来对样本进行划分。基尼系数越小，越稳定。</p>
<p>假设 t 为节点，那么该节点的 GINI 系数的计算公式为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/f9/89/f9bb4cce5b895499cabc714eb372b089.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。</p>
<p>通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：</p>
<p>集合 1：6 个都去打篮球；</p>
<p>集合 2：3 个去打篮球，3 个不去打篮球。</p>
<p>针对集合 1，所有人都去打篮球，所以 p(Ck|t)=1，因此 GINI(t)=1-1=0。</p>
<p>针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5<em>0.5+0.5</em>0.5）=0.5。</p>
<p>通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。</p>
<p>在 CART 算法中，基于基尼系数对特征属性进行二元分裂。</p>
<p>假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/69/9a/69a90a43146898150a0de0811c6fef9a.jpg" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/10/1e/107fed838cb75df62eb149499db20c1e.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。</p>
<h3 id="分类决策树"><a href="#分类决策树" class="headerlink" title="分类决策树"></a>分类决策树</h3><p>我们使用python中的 sklearn库来完成CART分类决策树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">iris=load_iris()</span><br><span class="line"><span class="comment"># 获取特征集和分类标识</span></span><br><span class="line">features = iris.data</span><br><span class="line">labels = iris.target</span><br><span class="line"><span class="comment"># 随机抽取 33% 的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=<span class="number">0.33</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 创建 CART 分类树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'gini'</span>)</span><br><span class="line"><span class="comment"># 拟合构造 CART 分类树</span></span><br><span class="line">clf = clf.fit(train_features, train_labels)</span><br><span class="line"><span class="comment"># 用 CART 分类树做预测</span></span><br><span class="line">test_predict = clf.predict(test_features)</span><br><span class="line"><span class="comment"># 预测结果与测试集结果作比对</span></span><br><span class="line">score = accuracy_score(test_labels, test_predict)</span><br><span class="line">print(<span class="string">"CART 分类树准确率 %.4lf"</span> % score)</span><br></pre></td></tr></table></figure>
<h3 id="回归决策树"><a href="#回归决策树" class="headerlink" title="回归决策树"></a>回归决策树</h3><p>CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。</p>
<p>样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。</p>
<p>其中差值的绝对值为样本值减去样本均值的绝对值：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/6f/97/6f9677a70b1edff85e9e467f3e52bd97.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>方差为每个样本值减去样本均值的平方和除以样本个数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/04/c1/045fd5afb7b53f17a8accd6f337f63c1.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>所以<strong>这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）</strong>。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。</p>
<p>我们可以通过一个例子来看下如何创建一棵 CART 回归树来做预测。</p>
<p>这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。</p>
<p>根据这些指标，我们使用 CART 回归树对波士顿房价进行预测，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_absolute_error,mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">boston=load_boston()</span><br><span class="line"><span class="comment"># 探索数据</span></span><br><span class="line">print(boston.feature_names)</span><br><span class="line"><span class="comment"># 获取特征集和房价</span></span><br><span class="line">features = boston.data</span><br><span class="line">prices = boston.target</span><br><span class="line"><span class="comment"># 随机抽取 33% 的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=<span class="number">0.33</span>)</span><br><span class="line"><span class="comment"># 创建 CART 回归树</span></span><br><span class="line">dtr=DecisionTreeRegressor()</span><br><span class="line"><span class="comment"># 拟合构造 CART 回归树</span></span><br><span class="line">dtr.fit(train_features, train_price)</span><br><span class="line"><span class="comment"># 预测测试集中的房价</span></span><br><span class="line">predict_price = dtr.predict(test_features)</span><br><span class="line"><span class="comment"># 测试集的结果评价</span></span><br><span class="line">print(<span class="string">'回归树二乘偏差均值:'</span>, mean_squared_error(test_price, predict_price))</span><br><span class="line">print(<span class="string">'回归树绝对值偏差均值:'</span>, mean_absolute_error(test_price, predict_price))</span><br></pre></td></tr></table></figure>
<h2 id="CART-决策树的剪枝"><a href="#CART-决策树的剪枝" class="headerlink" title="CART 决策树的剪枝"></a>CART 决策树的剪枝</h2><p>CART 决策树的剪枝主要采用的是 CCP 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/6b/95/6b9735123d45e58f0b0afc7c3f68cd95.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。</p>
<p>所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。</p>
<p>因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。</p>
<p>得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最終更新：<time datetime="2019-02-23T20:04:12.231Z" itemprop="dateUpdated">2019-02-24 04:04:12</time>
</span><br>


        
        Thanks for your reading  :) | URL <a href="/2019/02/18/机器学习-决策树/" target="_blank" rel="external">https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/</a>
        
    </div>
    
    <footer>
        <a href="https://joshuaqyh.github.io">
            <img src="/img/avatar.jpg" alt="Qiuyihao">
            Qiuyihao
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据分析/">数据分析</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/&title=《机器学习 | 决策树》 — KnowMyself&pic=https://joshuaqyh.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/&title=《机器学习 | 决策树》 — KnowMyself&source=Here are some records for life and study." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习 | 决策树》 — KnowMyself&url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/&via=https://joshuaqyh.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/02/18/机器学习-朴素贝叶斯分类/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">机器学习 | 朴素贝叶斯分类</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/02/17/python可视化实例-matplotlib-seaborn/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">10 个 python可视化实例 | matplotlib &amp; seaborn</h4>
      </a>
    </div>
  
</nav>



    

















</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>このブログの内容物は<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja">クリエイティブ・コモンズ 表示 - 非営利 - 継承 4.0 国際ライセンスの下に提供されています</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Qiuyihao &copy; 2017 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/&title=《机器学习 | 决策树》 — KnowMyself&pic=https://joshuaqyh.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/&title=《机器学习 | 决策树》 — KnowMyself&source=Here are some records for life and study." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习 | 决策树》 — KnowMyself&url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/&via=https://joshuaqyh.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://joshuaqyh.github.io/2019/02/18/机器学习-决策树/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACLUlEQVR42u3a0W6DMAwF0P7/TzNpT5Va0WubTiM5eUIbhBweXMfO4xGP43c8Xz+P1ztf/37+7Pmclw0MDIzbMo7Tcc6oztl7bzIbBgbGDowkyPb+m89w/vk+rBkDAwOjmLTly0qCOwYGBsZVATff0J4/i4GBgTHZxOb3XFXC+8peHAMD44aM3iv/5vrr/Q0MDIx/zzgGIyml5SljHmTfzIyBgbE0o5fSTbaj8/ujsIuBgbEQoxrUqktJnkpC7YePiIGBsQEjL+Xnhf7qEbHqz8CbpBADA2NpRi/k9VqY5bw1SV4xMDC2YSSLTgptk2ZAs5GJgYGxNCOfYh5w861s9VkMDIy1GXkAzSvwkxJ/MxnFwMBYmlFN0arJYi/ha/4MYGBgLM1IDlLMq1ujpQwaDBgYGOsx8qZm3sKcLC65xsDA2IfRa0lO5undX/iVwMDAWILRazTmr88Pll1wLAwDA2NRRpIU5oX+89TtqlZBM7fFwMBYiFFNB8snO4oFvnKIx8DAWJRRLfQnCV9y5CsJ34WAi4GBsTEjCXl5max3pCzaxGJgYCzNyJuR5fMaxQ/Ra3liYGCszTiKoxz4BmE6CcQYGBg7MPKRMHrFsupTzbMkGBgYN2f0NpbVZkDehuyljBgYGDswepvP5EhEtS3a5GFgYGB8gXEOKxfmMDAwMIrFuF7ZbhSmMTAwNmBUt6a9kllS+s+PbmBgYOzD6B2YyMtz8+TygmIcBgbGXRk/66HwLlLxz/8AAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '唉要去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
