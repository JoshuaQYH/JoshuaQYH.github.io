<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>机器学习 | 朴素贝叶斯分类 | KnowMyself | QiuYH&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="python,机器学习,数据分析,数据挖掘">
    <meta name="description" content="贝叶斯原理基本概念贝叶斯原理其实是在求解一个逆向概率的问题。 我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？ 你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那">
<meta name="keywords" content="python,机器学习,数据分析,数据挖掘">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习 | 朴素贝叶斯分类">
<meta property="og:url" content="https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/index.html">
<meta property="og:site_name" content="KnowMyself">
<meta property="og:description" content="贝叶斯原理基本概念贝叶斯原理其实是在求解一个逆向概率的问题。 我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？ 你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/31/5e/3163faf3b511e61408b46053aad7825e.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/99/4b/99f0e50baffa2c572ea0db6c5df4474b.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/8d/fa/8d16d796670bb4901c7a4c13ca3aa1fa.jpg">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/ac/3f/acd7a8e882bf0205f9b33c43fd61453f.jpg">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/bc/4d/bc31ff1f31f9cd26144404221f705d4d.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/b7/65/b7ad53560f61407e6964e7436da14365.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/63/12/63abe3ce8aa0ea4a78ba537b5504df12.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/b5/7e/b5ac88c4e2a71cc2d4ceef4c01e0ba7e.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/3b/8d/3bbe56a7b76513604bfe6b39b890dd8d.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/1e/2e/1e8b7465b9949fe071e95aede172a52e.png">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/25/c3/257e01f173e8bc78b37b71b2358ff7c3.jpg">
<meta property="og:updated_time" content="2019-02-23T19:50:43.436Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习 | 朴素贝叶斯分类">
<meta name="twitter:description" content="贝叶斯原理基本概念贝叶斯原理其实是在求解一个逆向概率的问题。 我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？ 你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那">
<meta name="twitter:image" content="https://static001.geekbang.org/resource/image/31/5e/3163faf3b511e61408b46053aad7825e.png">
    
        <link rel="alternate" type="application/atom+xml" title="KnowMyself" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Qiuyihao</h5>
          <a href="mailto:576261090@qq.com" title="576261090@qq.com" class="mail">576261090@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/JoshuaQYH" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">机器学习 | 朴素贝叶斯分类</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">机器学习 | 朴素贝叶斯分类</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-02-18T11:27:58.000Z" itemprop="datePublished" class="page-time">
  2019-02-18
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#贝叶斯原理"><span class="post-toc-number">1.</span> <span class="post-toc-text">贝叶斯原理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#基本概念"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">基本概念</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#贝叶斯公式"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">贝叶斯公式</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#朴素贝叶斯算法"><span class="post-toc-number">2.</span> <span class="post-toc-text">朴素贝叶斯算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#贝叶斯原理-贝叶斯分类-朴素贝叶斯之间的区别"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">贝叶斯原理 贝叶斯分类 朴素贝叶斯之间的区别</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#朴素贝叶斯分类器工作流程"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">朴素贝叶斯分类器工作流程</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#利用朴素贝叶斯进行文档分类"><span class="post-toc-number">3.</span> <span class="post-toc-text">利用朴素贝叶斯进行文档分类</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#sklearn-机器学习包"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">sklearn 机器学习包</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#TF-IDF"><span class="post-toc-number">3.1.1.</span> <span class="post-toc-text">TF-IDF</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#使用sklearn-来计算-TF-IDF"><span class="post-toc-number">3.1.2.</span> <span class="post-toc-text">使用sklearn 来计算 TF-IDF</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#对文档进行分类"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">对文档进行分类</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#分词"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">分词</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#加载停用词表"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">加载停用词表</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#计算单词的权重-TF-IDF"><span class="post-toc-number">3.2.3.</span> <span class="post-toc-text">计算单词的权重 TF-IDF</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#生成朴素贝叶斯分类器"><span class="post-toc-number">3.2.4.</span> <span class="post-toc-text">生成朴素贝叶斯分类器</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#使用生成的分类器做预测"><span class="post-toc-number">3.2.5.</span> <span class="post-toc-text">使用生成的分类器做预测</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#计算准确率"><span class="post-toc-number">3.2.6.</span> <span class="post-toc-text">计算准确率</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-机器学习-朴素贝叶斯分类" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">机器学习 | 朴素贝叶斯分类</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-02-18 19:27:58" datetime="2019-02-18T11:27:58.000Z" itemprop="datePublished">2019-02-18</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="贝叶斯原理"><a href="#贝叶斯原理" class="headerlink" title="贝叶斯原理"></a>贝叶斯原理</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>贝叶斯原理其实是在求解一个逆向概率的问题。</p>
<p>我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？</p>
<p>你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那是不是实际上患“贝叶死”的概率也是 99.9% 呢？实际上不是的。你自己想想，在 10000 个人中，还存在 0.1% 的误查的情况，也就是 10 个人没有患病但是被诊断成阳性。当然 10000 个人中，也确实存在一个患有贝叶死的人，他有 99.9% 的概率被检查出来。所以你可以粗算下，患病的这个人实际上是这 11 个人里面的一员，即实际患病比例是 1/11≈9%。</p>
<p>以上涉及了贝叶斯原理的几个核心概念：</p>
<ul>
<li>先验概率：通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。</li>
<li>后验概率：后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。</li>
<li>条件概率：事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。</li>
<li>联合概率：多个事件同时发生的概率。</li>
<li>联合分布：多个事件发生多个结果的概率分布情况。</li>
<li>似然函数：你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。</li>
</ul>
<h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a><strong>贝叶斯公式</strong></h2><p>一般的二元贝叶斯公式如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/31/5e/3163faf3b511e61408b46053aad7825e.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由此，我们可以得出通用的贝叶斯公式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/99/4b/99f0e50baffa2c572ea0db6c5df4474b.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>其实贝叶斯原理的可以通过条件概率公式来推理得到。</p>
<p>$P(Y|X)P(X)=P(X|Y)P(Y)=P(XY)P(Y|X)P(X)=P(X|Y)P(Y)=P(XY)$</p>
<p>放在实例中来观察贝叶斯公式的简单应用。在医疗诊断中，如果医生知道某一疾病发生某些症状的概率，那么可以利用贝叶斯公式估计得知当病人发生某症状时，推测病人发生某病的概率。</p>
<p>贝叶斯公式其实是反映了原因和结果之间的概率关系。</p>
<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><p>朴素贝叶斯算法是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，<strong>是因为它假设每一个输入变量之间是独立的。</strong></p>
<p>朴素贝叶斯模型由两种类型的概率组成：</p>
<ol>
<li>每个<strong>类别的概率</strong>$P(C_j)$；</li>
<li>每个属性的<strong>条件概率</strong>$P(Ai|C_j)$。</li>
</ol>
<p>朴素贝叶斯的公式如下：</p>
<p>$P(Cause,Effect_1,Effect_2,Effect_3….Effect_n)=P(Cause)∏_nP(Effect_i|Cause)$</p>
<p>为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。</p>
<h2 id="贝叶斯原理-贝叶斯分类-朴素贝叶斯之间的区别"><a href="#贝叶斯原理-贝叶斯分类-朴素贝叶斯之间的区别" class="headerlink" title="贝叶斯原理 贝叶斯分类 朴素贝叶斯之间的区别"></a>贝叶斯原理 贝叶斯分类 朴素贝叶斯之间的区别</h2><p>贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/8d/fa/8d16d796670bb4901c7a4c13ca3aa1fa.jpg" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<h2 id="朴素贝叶斯分类器工作流程"><a href="#朴素贝叶斯分类器工作流程" class="headerlink" title="朴素贝叶斯分类器工作流程"></a>朴素贝叶斯分类器工作流程</h2><p>朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。</p>
<p>流程可以用下图表示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/ac/3f/acd7a8e882bf0205f9b33c43fd61453f.jpg" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>从图片你也可以看出来，朴素贝叶斯分类器需要三个流程，我来给你一一讲解下这几个流程。</p>
<p><strong>第一阶段：准备阶段</strong></p>
<p>在这个阶段我们需要确定特征属性，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当划分，然后由人工对一部分数据进行分类，形成训练样本。</p>
<p>这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。</p>
<p><strong>第二阶段：训练阶段</strong></p>
<p>这个阶段就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。</p>
<p>输入是特征属性和训练样本，输出是分类器。</p>
<p><strong>第三阶段：应用阶段</strong></p>
<p>这个阶段是使用分类器对新数据进行分类。输入是分类器和新数据，输出是新数据的分类结果。</p>
<h1 id="利用朴素贝叶斯进行文档分类"><a href="#利用朴素贝叶斯进行文档分类" class="headerlink" title="利用朴素贝叶斯进行文档分类"></a>利用朴素贝叶斯进行文档分类</h1><p>朴素贝叶斯分类最适合的场景就是<strong>文本分类、情感分析和垃圾邮件识别</strong>。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。</p>
<p>以下是高斯朴素贝叶斯分类器的类简单实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.model = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求一组数据的数学期望，静态方法无需传入self</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mean</span><span class="params">(X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sum(X) / float(len(X))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求一组数据的标准差（方差）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stdev</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        avg = self.mean(X)</span><br><span class="line">        <span class="keyword">return</span> math.sqrt(sum([pow(x-avg, <span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> X]) / float(len(X)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 概率密度函数。正态分布函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gaussian_probability</span><span class="params">(self, x, mean, stdev)</span>:</span></span><br><span class="line">        exponent = math.exp(-(math.pow(x-mean,<span class="number">2</span>)/(<span class="number">2</span>*math.pow(stdev,<span class="number">2</span>))))</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1</span> / (math.sqrt(<span class="number">2</span>*math.pi) * stdev)) * exponent</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理X_train，得到均值和标准差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">summarize</span><span class="params">(self, train_data)</span>:</span></span><br><span class="line">        summaries = [(self.mean(i), self.stdev(i)) <span class="keyword">for</span> i <span class="keyword">in</span> zip(*train_data)]</span><br><span class="line">        <span class="keyword">return</span> summaries</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分类别求出数学期望和标准差，然后拟合。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        labels = list(set(y))</span><br><span class="line">        data = &#123;label:[] <span class="keyword">for</span> label <span class="keyword">in</span> labels&#125;</span><br><span class="line">        <span class="keyword">for</span> f, label <span class="keyword">in</span> zip(X, y):</span><br><span class="line">            data[label].append(f)</span><br><span class="line">        self.model = &#123;label: self.summarize(value) <span class="keyword">for</span> label, value <span class="keyword">in</span> data.items()&#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'gaussianNB train done!'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_probabilities</span><span class="params">(self, input_data)</span>:</span></span><br><span class="line">        <span class="comment"># summaries:&#123;0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]&#125;</span></span><br><span class="line">        <span class="comment"># input_data:[1.1, 2.2]</span></span><br><span class="line">        probabilities = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> label, value <span class="keyword">in</span> self.model.items():</span><br><span class="line">            probabilities[label] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(value)):</span><br><span class="line">                mean, stdev = value[i]</span><br><span class="line">                probabilities[label] *= self.gaussian_probability(input_data[i], mean, stdev)</span><br><span class="line">        <span class="keyword">return</span> probabilities</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 类别</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_test)</span>:</span></span><br><span class="line">        <span class="comment"># &#123;0.0: 2.9680340789325763e-27, 1.0: 3.5749783019849535e-26&#125;</span></span><br><span class="line">        label = sorted(self.calculate_probabilities(X_test).items(), key=<span class="keyword">lambda</span> x: x[<span class="number">-1</span>])[<span class="number">-1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self，X_test, y_test)</span>:</span></span><br><span class="line">        right = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> zip(X_test, y_test):</span><br><span class="line">            label = self.predict(X)</span><br><span class="line">            <span class="keyword">if</span> label == y:</span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> right / float(len(X_test))</span><br></pre></td></tr></table></figure>
<p>此外，我们直接使用 sklearn 机器学习包来帮助我们使用朴素贝叶斯分类算法。</p>
<h2 id="sklearn-机器学习包"><a href="#sklearn-机器学习包" class="headerlink" title="sklearn 机器学习包"></a>sklearn 机器学习包</h2><p>sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。</p>
<p>这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：</p>
<p><strong>高斯朴素贝叶斯</strong>：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。</p>
<p><strong>多项式朴素贝叶斯</strong>：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。</p>
<p><strong>伯努利朴素贝叶斯</strong>：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。</p>
<p>伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。</p>
<p><strong>词频 TF</strong>计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。</p>
<p><strong>逆向文档频率 IDF</strong>，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。</p>
<p><strong>所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积</strong>。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，<strong>即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类</strong>。</p>
<p>首先我们看下词频 TF 和逆向文档概率 IDF 的公式。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/bc/4d/bc31ff1f31f9cd26144404221f705d4d.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/b7/65/b7ad53560f61407e6964e7436da14365.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。</p>
<p><strong>TF-IDF=TF*IDF。</strong></p>
<p>你可以看到，TF-IDF 值就是 TF 与 IDF 的乘积, 这样可以更准确地对文档进行分类。比如“我”这样的高频单词，虽然 TF 词频高，但是 IDF 值很低，整体的 TF-IDF 也不高。</p>
<p>我在这里举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。</p>
<p>针对“this”，计算 TF-IDF 值：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/63/12/63abe3ce8aa0ea4a78ba537b5504df12.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/b5/7e/b5ac88c4e2a71cc2d4ceef4c01e0ba7e.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>所以 TF-IDF=0.02*(-0.0414)=-8.28e-4。</p>
<p>针对“bayes”，计算 TF-IDF 值：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/3b/8d/3bbe56a7b76513604bfe6b39b890dd8d.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/1e/2e/1e8b7465b9949fe071e95aede172a52e.png" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<p>TF-IDF=0.005*0.5229=2.61e-3。</p>
<p>很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。</p>
<h3 id="使用sklearn-来计算-TF-IDF"><a href="#使用sklearn-来计算-TF-IDF" class="headerlink" title="使用sklearn 来计算 TF-IDF"></a>使用sklearn 来计算 TF-IDF</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a object to calculate TF-IDF</span></span><br><span class="line"><span class="comment"># TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)</span></span><br><span class="line"><span class="comment"># * stop_words 指的是：在分类中没有用的单词，对分类起不到作用，减少计算浪费。</span></span><br><span class="line"><span class="comment"># * token_pattern 使用正则表达式定义过滤规则</span></span><br><span class="line"></span><br><span class="line">tfidf_vec = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># the fitting data</span></span><br><span class="line">documents = [</span><br><span class="line">    <span class="string">'this is the bayes document'</span>,</span><br><span class="line">    <span class="string">'this is the second second document'</span>,</span><br><span class="line">    <span class="string">'and the third one'</span>,</span><br><span class="line">    <span class="string">'is this the document'</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># calcualte TF-IDF，拟合模型，返回TF—IDF计算结果矩阵。</span></span><br><span class="line">tfidf_matrix = tfidf_vec.fit_transform(documents)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'不重复的词:'</span>, tfidf_vec.get_feature_names())</span><br><span class="line">print(<span class="string">'每个单词的 ID:'</span>, tfidf_vec.vocabulary_)</span><br><span class="line">print(<span class="string">'每个单词的 tfidf 值:'</span>, tfidf_matrix.toarray())</span><br></pre></td></tr></table></figure>
<h2 id="对文档进行分类"><a href="#对文档进行分类" class="headerlink" title="对文档进行分类"></a>对文档进行分类</h2><p>如果我们要对文档进行分类，有两个重要的阶段：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://static001.geekbang.org/resource/image/25/c3/257e01f173e8bc78b37b71b2358ff7c3.jpg" alt="img" title="">
                </div>
                <div class="image-caption">img</div>
            </figure>
<ol>
<li><strong>基于分词的数据准备</strong>，包括分词、单词权重计算、去掉停用词；</li>
<li><strong>应用朴素贝叶斯分类进行分类</strong>，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。</li>
</ol>
<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>在准备阶段，分词是比较重要的一个环节。在英文文档中分词的工具为NLTK包，而中文文档则是使用了 jieba包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">word_list = nltk.word_tokenize(text) <span class="comment"># 分词</span></span><br><span class="line">nltk.pos_tag(word_list) <span class="comment"># 标注单词的词性</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word_list = jieba.cut (text) <span class="comment"># 中文分词</span></span><br></pre></td></tr></table></figure>
<h3 id="加载停用词表"><a href="#加载停用词表" class="headerlink" title="加载停用词表"></a>加载停用词表</h3><p>我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop_words = [line.strip().decode(<span class="string">'utf-8'</span>) <span class="keyword">for</span> line <span class="keyword">in</span> io.open(<span class="string">'stop_words.txt'</span>).readlines()]</span><br></pre></td></tr></table></figure>
<h3 id="计算单词的权重-TF-IDF"><a href="#计算单词的权重-TF-IDF" class="headerlink" title="计算单词的权重 TF-IDF"></a>计算单词的权重 TF-IDF</h3><p>直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>)</span><br><span class="line">features = tf.fit_transform(train_contents)</span><br></pre></td></tr></table></figure>
<p>这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。</p>
<h3 id="生成朴素贝叶斯分类器"><a href="#生成朴素贝叶斯分类器" class="headerlink" title="生成朴素贝叶斯分类器"></a>生成朴素贝叶斯分类器</h3><p>我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。</p>
<p>这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。</p>
<p>当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。</p>
<p>当 0&lt;alpha&lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多项式贝叶斯分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB  <span class="comment"># GuassianNB BernoulliNB</span></span><br><span class="line">clf = MultinomialNB(alpha=<span class="number">0.001</span>).fit(train_features, train_labels)</span><br></pre></td></tr></table></figure>
<h3 id="使用生成的分类器做预测"><a href="#使用生成的分类器做预测" class="headerlink" title="使用生成的分类器做预测"></a>使用生成的分类器做预测</h3><p>首先我们需要得到测试集的特征矩阵。</p>
<p>方法是用训练集的分词创建一个 TfidfVectorizer 类，使用同样的 stop_words 和 max_df，然后用这个 TfidfVectorizer 类对测试集的内容进行 fit_transform 拟合，得到测试集的特征矩阵 test_features。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>, vocabulary=train_vocabulary)</span><br><span class="line">test_features=test_tf.fit_transform(test_contents)</span><br></pre></td></tr></table></figure>
<p>然后我们用训练好的分类器对新数据做预测。</p>
<p>方法是使用 predict 函数，传入测试集的特征矩阵 test_features，得到分类结果 predicted_labels。predict 函数做的工作就是求解所有后验概率并找出最大的那个。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predicted_labels=clf.predict(test_features)</span><br></pre></td></tr></table></figure>
<h3 id="计算准确率"><a href="#计算准确率" class="headerlink" title="计算准确率"></a>计算准确率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> metrics.accuracy_score(test_labels, predicted_labels)</span><br></pre></td></tr></table></figure>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2019-02-23T19:50:43.436Z" itemprop="dateUpdated">2019-02-24 03:50:43</time>
</span><br>


        
        Thanks for your reading  :) | URL <a href="/2019/02/18/机器学习-朴素贝叶斯分类/" target="_blank" rel="external">https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/</a>
        
    </div>
    
    <footer>
        <a href="https://joshuaqyh.github.io">
            <img src="/img/avatar.jpg" alt="Qiuyihao">
            Qiuyihao
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据分析/">数据分析</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据挖掘/">数据挖掘</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/&title=《机器学习 | 朴素贝叶斯分类》 — KnowMyself&pic=https://joshuaqyh.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/&title=《机器学习 | 朴素贝叶斯分类》 — KnowMyself&source=Here are some records for life and study." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习 | 朴素贝叶斯分类》 — KnowMyself&url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/&via=https://joshuaqyh.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/02/18/机器学习-支持向量机SVM/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">机器学习 | 使用SVM进行乳腺癌检测</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/02/18/机器学习-决策树/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">机器学习 | 决策树</h4>
      </a>
    </div>
  
</nav>



    

















</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Qiuyihao &copy; 2017 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/&title=《机器学习 | 朴素贝叶斯分类》 — KnowMyself&pic=https://joshuaqyh.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/&title=《机器学习 | 朴素贝叶斯分类》 — KnowMyself&source=Here are some records for life and study." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习 | 朴素贝叶斯分类》 — KnowMyself&url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/&via=https://joshuaqyh.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://joshuaqyh.github.io/2019/02/18/机器学习-朴素贝叶斯分类/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACsElEQVR42u3awWrkMBAE0Pz/T2dhTxuIN1XdUiaB55MRk5GeA5amut/e4uv97/Xv/dNIMp5/89MnD194eHh4o6U/XTnpaXHJSDI+mx0PDw/vHm+2Gcwe0P/B+XqiB4SHh4f3Ul6C2Ry4240BDw8P77fw2sUlx+L9evDw8PC+n5e83Nuod3ZA329OeHh4eLd5swLYa++v1/fw8PDwRlX1tvA/OzrnW1G9Wjw8PLwLvCRmzd+9+bLyefODcvGLAQ8PD2/B20zQlsRmAXHbdvBhBA8PD+8aLwkgZuOnjsv5LvfJLwY8PDy8o7w2+cxf5e1Dactjw3IdHh4e3iHepvh0YzwPRKKoAg8PD+8ory1xtQFr/opvMdHxHQ8PD+8ab9961U6fz9L+bbQL4eHh4S14+Sv4RnGrDS/aUhweHh7ePV5b0Nocu5OGg3wN0SPAw8PDO8rbN49uWqPaAlgy+4d7PDw8vAu8vBjfRrT5fR3Olo8GDw8P7yzvXqjahrYJuy6k4eHh4V3gzQLWPFTdY5K2huG+h4eHhzfitQfZfBGnRlYbFR4eHt4F3uYr2oeSh8KbhgM8PDy8n8A7taBZo0B+TK97yvDw8PBGvHbitjw2O/K+l9fwi/Dw8PBGvLaVarMZDAtX++IcHh4e3lHepujVBhZ5KJwDigIYHh4e3gVeezietQ7k7LZNoajv4eHh4S14eYCbUDebR/4Nq+ADDw8Pb83bB6azCDh5fPmB/nENeHh4eBd4Z/uU8pggihKCeY+1ZOHh4eGVvM3E+efPtiAU/yU8PDy8a7x2M9g3GeThRbuBfdIZgYeHh/ci3iYOSMKINnWOPomHh4f3A3ibMLcNOPJmgsfZ8fDw8K7xNgFrcrTN8XmDQrQ2PDw8vAu8fQFsf2jOD+5fRA+zCw8PDy/l/QHG6/k+enw26wAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '唉要去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
