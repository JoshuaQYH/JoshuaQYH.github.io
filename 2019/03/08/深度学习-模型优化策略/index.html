<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>深度学习模型中的优化策略 | KnowMyself | QiuYH&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="深度学习">
    <meta name="description" content="在深度学习中，我们通常需要优化我们所定义的损失函数，优化的方法思想就是通过梯度下降的方式不断逼近一个局部最优解，更新模型参数然后来保证损失函数预测误差最小。在本文主要综述了几种常见的优化策略，包括梯度下降的策略以及学习率变化的策略。这些策略有一些基本的概念需要重点掌握，这些策略有简单有复杂， 但是依照“没有免费午餐“的定理，在具体情景中，要具体运用比较，没有绝对的最优策略，只有相对的最优策略">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习模型中的优化策略">
<meta property="og:url" content="https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/index.html">
<meta property="og:site_name" content="KnowMyself">
<meta property="og:description" content="在深度学习中，我们通常需要优化我们所定义的损失函数，优化的方法思想就是通过梯度下降的方式不断逼近一个局部最优解，更新模型参数然后来保证损失函数预测误差最小。在本文主要综述了几种常见的优化策略，包括梯度下降的策略以及学习率变化的策略。这些策略有一些基本的概念需要重点掌握，这些策略有简单有复杂， 但是依照“没有免费午餐“的定理，在具体情景中，要具体运用比较，没有绝对的最优策略，只有相对的最优策略">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://raw.githubusercontent.com/JoshuaQYH/blogImage/master/img/20190308021211.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JoshuaQYH/blogImage/master/img/20190308022822.png">
<meta property="og:updated_time" content="2019-03-07T18:35:14.513Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习模型中的优化策略">
<meta name="twitter:description" content="在深度学习中，我们通常需要优化我们所定义的损失函数，优化的方法思想就是通过梯度下降的方式不断逼近一个局部最优解，更新模型参数然后来保证损失函数预测误差最小。在本文主要综述了几种常见的优化策略，包括梯度下降的策略以及学习率变化的策略。这些策略有一些基本的概念需要重点掌握，这些策略有简单有复杂， 但是依照“没有免费午餐“的定理，在具体情景中，要具体运用比较，没有绝对的最优策略，只有相对的最优策略">
<meta name="twitter:image" content="https://raw.githubusercontent.com/JoshuaQYH/blogImage/master/img/20190308021211.png">
    
        <link rel="alternate" type="application/atom+xml" title="KnowMyself" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Qiuyihao</h5>
          <a href="mailto:576261090@qq.com" title="576261090@qq.com" class="mail">576261090@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/JoshuaQYH" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">深度学习模型中的优化策略</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">深度学习模型中的优化策略</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-03-07T16:15:32.000Z" itemprop="datePublished" class="page-time">
  2019-03-08
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#批量梯度下降法-Batch-gradient-descent"><span class="post-toc-number">1.</span> <span class="post-toc-text">批量梯度下降法 Batch gradient descent</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#随机梯度下降法-Stochastic-gradient-descent"><span class="post-toc-number">2.</span> <span class="post-toc-text">随机梯度下降法 Stochastic gradient descent</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#小批量梯度下降-Mini-batch-gradient-descent"><span class="post-toc-number">3.</span> <span class="post-toc-text">小批量梯度下降 Mini-batch gradient descent</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#动量-Momentum"><span class="post-toc-number">4.</span> <span class="post-toc-text">动量 Momentum</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#自适应梯度下降法-Adagrad"><span class="post-toc-number">5.</span> <span class="post-toc-text">自适应梯度下降法 Adagrad</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#RMSprop-root-mean-square-propagation"><span class="post-toc-number">6.</span> <span class="post-toc-text">RMSprop (root mean square propagation)</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Adam-Adaptive-moment-estimation"><span class="post-toc-number">7.</span> <span class="post-toc-text">Adam (Adaptive moment estimation)</span></a></li></ol>
        </nav>
    </aside>


<article id="post-深度学习-模型优化策略" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">深度学习模型中的优化策略</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-03-08 00:15:32" datetime="2019-03-07T16:15:32.000Z" itemprop="datePublished">2019-03-08</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>   在深度学习中，我们通常需要优化我们所定义的损失函数，优化的方法思想就是通过梯度下降的方式不断逼近一个局部最优解，更新模型参数然后来保证损失函数预测误差最小。在本文主要综述了几种常见的优化策略，包括梯度下降的策略以及学习率变化的策略。这些策略有一些基本的概念需要重点掌握，这些策略有简单有复杂， 但是依照“没有免费午餐“的定理，在具体情景中，要具体运用比较，没有绝对的最优策略，只有相对的最优策略。</p>
<p>虽然在实践中我们可以直接调用深度学习框架的API来完成优化，但是要更加深入地理解其原理和过程才是一个优秀的深度学习工程师啊，面试肯定会涉及一些原理概念解释滴。下面就开始总结啦！</p>
<h1 id="批量梯度下降法-Batch-gradient-descent"><a href="#批量梯度下降法-Batch-gradient-descent" class="headerlink" title="批量梯度下降法 Batch gradient descent"></a>批量梯度下降法 Batch gradient descent</h1><p>批量梯度下降法（也叫确定性梯度算法）指的是在<strong>一个大批量的数据样本中同时处理所有样本</strong>，在更新参数的时候，会同时利用所有样本的梯度变化。公式表达如下：</p>
<p>$$\theta_{t+1} = \theta_{t} - \eta \bigtriangledown L(\theta_t)  = \theta_t - \frac{\eta}{N} \sum_{n=1}^{N}\bigtriangledown l(y_n, f(x_n; \theta_t))$$  </p>
<p>从公式上可以看到模型参数$\theta$开始更新的时候，是当计算得到所有样本$x_n$输出预测结果$f(x_n; \theta_t)$后得到损失函数梯度值时，进行模型参数的更新。</p>
<p>可以看到这整个过程的缺点就是训练慢，处理棘手，而且内存消耗大，但是可以保证找到一个最小的损失函数值。</p>
<p>伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for iter in range(nb_epoches):</span><br><span class="line">	params_grad = eval_grad(loss_func, dataset, params)</span><br><span class="line">	params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<h1 id="随机梯度下降法-Stochastic-gradient-descent"><a href="#随机梯度下降法-Stochastic-gradient-descent" class="headerlink" title="随机梯度下降法 Stochastic gradient descent"></a>随机梯度下降法 Stochastic gradient descent</h1><p>随机梯度下降法简称SGD，与批量梯度下降法相比，随机梯度下降法是在每训练一个样本的时候就进行的函数模型的更新，在每次的训练迭代中，会先随机打乱训练集，然后每抽取一个样本进行训练时，就进行更新。</p>
<p>公式表达如下：</p>
<p>$\theta_{t+1} = \theta_t - \eta \bigtriangledown L(\theta_t) = \theta_t - \eta \bigtriangledown l(y_n , f(x_n; \theta_t))$</p>
<p>显而易见，在处理大数据样本的时候，模型更新速度很快，但是存在收敛震荡的情况，可能会跳出局部最优解，但是也很接近局部最优解。从计算设备的角度来考虑，随机梯度下降无法做到很好的并行计算，没有很好地利用多核架构，每一次更新都得依赖前一个样本的计算。</p>
<p>伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for iter in range(epoches):</span><br><span class="line">	np.random.shuffle(dataset)</span><br><span class="line">	for example in dataset:</span><br><span class="line">		param_grad = eval_grad(loss_func, example, params)</span><br><span class="line">		params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<h1 id="小批量梯度下降-Mini-batch-gradient-descent"><a href="#小批量梯度下降-Mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降 Mini-batch gradient descent"></a>小批量梯度下降 Mini-batch gradient descent</h1><p>小批量梯度下降法，其实就是介于批量下降法和随机梯度下降法之间的优化方法，关键在于每次更新模型参数时，训练样本的大小。我们知道大批量的运算将会使我们的计算设备和内存不堪重负，而随机梯度下降法的中的单样本更新则无法充分使用我们的多核计算架构。我们想要达到的目的就是：<strong>既能有效的保证计算设备的良好充分使用，又能保证较短的训练时间和较好的性能</strong> 。而小批量梯度下降法就是我们比较常用的一种优化方法。</p>
<p>小批量的大小通常使用以下几个因素决定的：</p>
<ul>
<li>更大的批量将计算得到更为精确的梯度估计，但是回报却是小于线性的。</li>
<li>极小批量将无法充分利用多核架构，这促使我们给批量设定一个最小阈值。</li>
<li>如果批量处理中所有样本可以并行地处理，那么内存消耗和批量大小将会呈正比，所有批量有一个上限。</li>
</ul>
<p>小批量随机梯度下降法用数学公式表达如下：</p>
<p>$$\theta_{t+1} = \theta_{t} - \eta\bigtriangledown L(\theta_t) = \theta_t - \frac{\eta}{S} \sum_{(x_n, y_n) ∈S} \bigtriangledown l(y_n, f(x_n; \theta_t))$$</p>
<p>S 代表批量的大小，通常为10~300之间。</p>
<p>整个过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for i in range(nb_epochs):</span><br><span class="line">	np.random.shuffle(dataset)</span><br><span class="line">	for batch in get_batches(dataset, batch_size = 64):</span><br><span class="line">		params_grad = eval_grad(loss_func, batch, params)</span><br><span class="line">		params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<p>对于小批量梯度下降法来说，用小批量可以比较好的近似真实梯度，但是对学习率的自适应调整有了一定的要求。</p>
<h1 id="动量-Momentum"><a href="#动量-Momentum" class="headerlink" title="动量 Momentum"></a>动量 Momentum</h1><p>虽然SGD仍然是非常接收欢迎的优化方法，但是其学习过程有时是相当的漫长，动量方法就是为了加速学习，特别是在处理高曲率，小但一致的梯度，或者带噪声的梯度。动量算法简单来说就是在原来SGD的基础上，利用了之前所有梯度指数级衰减的移动平均，并且沿该方向继续移动。</p>
<p>在描述该方法的时候，引入了变量 v 来充当速度的角色，它代表参数在参数空间移动的方向和速率。数学描述如下：</p>
<p>$v_{t+1} = \gamma v_t + \eta\bigtriangledown L(\theta_t)$</p>
<p>$\theta_{t+1} = \theta_t - v_{t+1}$</p>
<p>该迭代式可以看出：</p>
<p>$v_{t+1} = \eta { \bigtriangledown L(\theta_t) + \eta L(\theta_{t-1}) + \eta^2 \bigtriangledown L(\theta_{t-2}）+ … }$</p>
<p>这种方法有效的利用了前面所有的梯度信息，可以有效的减少SGD，mini-batch的震荡，但是这种算法有时会导致跳出最优解，而且随着计算次数的增加，有些梯度信息已经几乎可以忽略不记了。</p>
<h1 id="自适应梯度下降法-Adagrad"><a href="#自适应梯度下降法-Adagrad" class="headerlink" title="自适应梯度下降法 Adagrad"></a>自适应梯度下降法 Adagrad</h1><p>自适应可以理解为学习率的自适应变化，在基础的下降法种的学习率$\eta$ 是一个常数，在自适应法中，学习率需要除以之前所有导数的平方和的平方根。</p>
<p>表达式如下：</p>
<p>$g_{t+1, i} = g_{t, i} + (\frac{\partial L(\theta_t)}{\partial \theta_i})^2$</p>
<p>$\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{g_{t+1, i}} + \epsilon} \frac{\partial L(\theta_t)}{\partial  \theta_i}$</p>
<p>该方法实现了学习率的自动调整，当迭代次数增加的时候，学习率将会变得极小。</p>
<h1 id="RMSprop-root-mean-square-propagation"><a href="#RMSprop-root-mean-square-propagation" class="headerlink" title="RMSprop (root mean square propagation)"></a>RMSprop (root mean square propagation)</h1><p>为了解决Adagrad学习率单调递减的原因，我们对于每一个时刻的导数计算，引入了$\gamma $来避免学习率极低的情况，是简单利用线性加权的方式来描述这一方法的：</p>
<p>$g_{t+1, i} = \gamma g_{t, i} + (1 - \gamma)(\frac{\partial L(\theta_t)}{\partial \theta_i}) ^2$</p>
<p>$\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{g_{t+1, i} + \epsilon}} \frac{\partial L(\theta_t)}{\partial \theta_i}$</p>
<p>一般$gamma$可以取0.9。</p>
<h1 id="Adam-Adaptive-moment-estimation"><a href="#Adam-Adaptive-moment-estimation" class="headerlink" title="Adam (Adaptive moment estimation)"></a>Adam (Adaptive moment estimation)</h1><p>Adam 简单来说就是在RMSProp的基础之上，添加了momentum和bias的方法，过程如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://raw.githubusercontent.com/JoshuaQYH/blogImage/master/img/20190308021211.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p><img src="https://raw.githubusercontent.com/JoshuaQYH/blogImage/master/img/20190308022822.png" alt=""></p>
<p>从上文可以看到，Adam其实是对学习率和要更新的梯度值进行了修改，学习率近似于RMSProp的方法，而梯度的修改则是参照了Momentum方法，当前的梯度信息需要依赖以往所有梯度信息的线性加权求和，权重随着迭代次数增加而减少。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2019-03-07T18:35:14.513Z" itemprop="dateUpdated">2019-03-08 02:35:14</time>
</span><br>


        
        Thanks for your reading  :) | URL <a href="/2019/03/08/深度学习-模型优化策略/" target="_blank" rel="external">https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/</a>
        
    </div>
    
    <footer>
        <a href="https://joshuaqyh.github.io">
            <img src="/img/avatar.jpg" alt="Qiuyihao">
            Qiuyihao
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/&title=《深度学习模型中的优化策略》 — KnowMyself&pic=https://joshuaqyh.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/&title=《深度学习模型中的优化策略》 — KnowMyself&source=Here are some records for life and study." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深度学习模型中的优化策略》 — KnowMyself&url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/&via=https://joshuaqyh.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/03/08/软件工程-软件质量标准/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">软件质量标准</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/03/05/机器学习-PageRank-算法和简单应用/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">机器学习 | PageRank 算法和简单应用</h4>
      </a>
    </div>
  
</nav>



    

















</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Qiuyihao &copy; 2017 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/&title=《深度学习模型中的优化策略》 — KnowMyself&pic=https://joshuaqyh.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/&title=《深度学习模型中的优化策略》 — KnowMyself&source=Here are some records for life and study." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深度学习模型中的优化策略》 — KnowMyself&url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/&via=https://joshuaqyh.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://joshuaqyh.github.io/2019/03/08/深度学习-模型优化策略/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACtUlEQVR42u3aQW7DMAwEwP7/0+m1h0ZZkmLhAuNTkLS2xgeRWPHrK75eP67z9683189fz/+b/Hr5wsPDwxss/bys/PvqSzmv54w/rwEPDw9vj/fuMefPyR3OG3pSDPIigYeHh/dM3gSc36H6ivHw8PD+C68aT1TvjIeHh/dkXjV+TRro/D69InE5a8HDw8OLefkp0nM+r5zv4eHh4Y1P1ZMN91wGku+r8W5htXh4eHgLvGrkOj/Qmg8K5OvEw8PD+xteHsvmIwLVsGPSG/9yNzw8PLyrvOpwQD5AkLfR11rn+ewDHh4eXouXt7a9Y7AksDivZFTx8PDw8Ma8altcXe481EjWUz4Mw8PDwxvz8qVXR6/yl5U05b1SgYeHh3eXVx0RyLfpedSb7PMf7oCHh4e3xsuj2+o2XR0UqBahD8UMDw8Pb4HXg+XbfS/Y7cXEH/4XDw8P7xKvd+iVHF9NxhGqW//b5+Lh4eEt8Ca36JWHyUhWb5QBDw8Pb5uXLLp3/N8rANdGCvDw8PAu8fJBgeTxhUGoPH6NC0YURuDh4eGNecnDkhb2FV+Toas8+MDDw8Pb403a6Gr4m2zo1Xi3OXSFh4eHN+b12tnqcVS1He+9+ma2gYeHh9fibZeHu6MDhWQaDw8P7yov+bkXIuQvJS8hvReKh4eHt83LBwLyg6hCllx8udFoAh4eHt4yb9JYz9vopGmuhsJ4eHh4f8/L99jeQnvHXYULDw8PbwE2GZnqHXHNS040jIWHh4e3wBuNNB3LQG/pd4NjPDw8vD3efCDgbnbaa8Gj+Qg8PDy8q7zJ4VNSJKrjAnnt+hDs4uHh4T2G14t9JyWhGQfj4eHhPZJXTT6qpSUPi6+11Hh4eHiDE/ZqENCLLZJFVyNgPDw8vD1ebzDr/AqSh/X+fsTAw8PDm/K+AVoV2k9sODCdAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '唉要去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
